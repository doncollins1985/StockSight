#!/usr/bin/env python
#================================
#  main.py
#================================
"""
Orchestrates the entire pipeline:
<<<<<<< HEAD
    1) Fetch stock data (yfinance + TA-Lib)  [subcommand: fetch]
    2) Fetch news data (NYT API)             [subcommand: news]
    3) Merge sentiment & create sequences    [subcommand: data]
    4) Train the model                       [subcommand: train]
    5) Evaluate the model                    [subcommand: evaluate]

Usage Examples:
    python main.py fetch --ticker ^GSPC --start 2025-01-01 --end 2025-06-01
    python main.py news --start 2025-01-01
    python main.py data --config config.json
=======
    1) Fetch stock data (stock.py)
    2) Merge sentiment & create sequences (data.py)
    3) Train the model (train.py)
    4) Evaluate the model (evaluate.py)

Usage:
    python main.py fetch --ticker ^GSPC --start 2020-01-01 --end 2021-01-01 ...
    python main.py data-setup --config config.json
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
    python main.py train --config config.json
    python main.py evaluate --config config.json
"""

import argparse
import logging
import sys
import os
<<<<<<< HEAD
import time
import json
import csv
from datetime import datetime
from typing import List

import requests
import pandas as pd

# Import your existing scripts as modules:
import scripts.stock as stock          # your stock.py
import scripts.data as data            # your data.py
import scripts.train as train          # your train.py
import scripts.evaluate as evaluate    # your evaluate.py

from scripts.utils import load_config
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import backoff
from ratelimit import limits, sleep_and_retry
from tqdm import tqdm

# ------------------------------------------------------------------------
# Global Logging Setup
# ------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
=======

# We import these so we can call their "main" or their key functions
import scripts.stock as stock          # stock.py
import scripts.data as data            # data.py
import scripts.train as train          # train.py
import scripts.evaluate as evaluate    # evaluate.py

from scripts.utils import load_config

logging.basicConfig(
    level=logging.WARN,
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

<<<<<<< HEAD
# ------------------------------------------------------------------------
# NEWS FETCHING LOGIC (formerly in news.py) 
# Now integrated as a subcommand "news"
# ------------------------------------------------------------------------

API_KEY = os.getenv("NYT_API_KEY")
if not API_KEY:
    logging.error("NYT_API_KEY environment variable not set.")
    raise EnvironmentError("Please set the NYT_API_KEY environment variable to access the NYT API.")

BASE_URL = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
QUERY = "finance business technology"
MAX_RETRIES = 2
BACKOFF_FACTOR = 2
MAX_PAGES = 10       # Maximum pages per date

# Rate limiting for the NYT API
RATE_LIMIT_CALLS = 8
RATE_LIMIT_PERIOD = 60  # 60 seconds

def create_session() -> requests.Session:
    """
    Create a requests Session with a retry strategy.
    """
    session = requests.Session()
    retries = Retry(
        total=MAX_RETRIES,
        backoff_factor=BACKOFF_FACTOR,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session

def is_valid_date(date_str: str) -> bool:
    """
    Check if the provided string is a valid date in 'YYYY-MM-DD' format.
    """
    try:
        datetime.strptime(date_str, "%Y-%m-%d")
        return True
    except ValueError:
        return False

def validate_start_date(start_date_str: str) -> datetime:
    """
    Validate and convert the start date string to a datetime object (YYYY-MM-DD).
    """
    try:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        return start_date
    except ValueError:
        logging.error("start_date must be in 'YYYY-MM-DD' format.")
        raise

def load_dates_from_stock_data(stock_file: str, start_date_dt: datetime) -> List[str]:
    """
    Load dates from a stock prices CSV file, filtering by dates >= start_date_dt.
    """
    try:
        stock_data = pd.read_csv(stock_file)
        if 'Date' not in stock_data.columns:
            logging.error("Stock data CSV does not contain 'Date' column.")
            raise KeyError("Missing 'Date' column.")

        # Convert date col to datetime objects
        valid_dates = []
        for d in stock_data['Date'].dropna().unique():
            if is_valid_date(str(d)):
                dt_obj = datetime.strptime(str(d), "%Y-%m-%d")
                if dt_obj >= start_date_dt:
                    valid_dates.append(dt_obj)
        valid_dates.sort()
        return [dt.strftime("%Y-%m-%d") for dt in valid_dates]
    except Exception as e:
        logging.error(f"Failed to load dates from {stock_file}: {e}")
        raise

def initialize_output_file(output_file: str) -> None:
    """
    Initialize the output CSV file by creating it with headers if it doesn't exist.
    """
    if not os.path.exists(output_file):
        try:
            with open(output_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['Date', 'Articles'])
            logging.info(f"Created new output file with headers: {output_file}")
        except Exception as e:
            logging.error(f"Failed to initialize output file {output_file}: {e}")
            raise

def get_existing_dates(output_file: str) -> set:
    """
    Retrieve set of dates already in the output CSV file.
    """
    existing_dates = set()
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    date_str = row.get('Date', '').strip()
                    if is_valid_date(date_str):
                        existing_dates.add(date_str)
            logging.info(f"Loaded {len(existing_dates)} existing dates from {output_file}.")
        except Exception as e:
            logging.error(f"Failed to read existing output file {output_file}: {e}")
    return existing_dates

def append_to_csv(output_file: str, date: str, articles: List[str]) -> None:
    """
    Append a single row (Date, JSON-encoded articles) to the CSV file.
    """
    articles_str = json.dumps(articles, ensure_ascii=False)
    try:
        with open(output_file, 'a', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([date, articles_str])
        logging.info(f"Appended {len(articles)} articles for date {date} -> {output_file}.")
    except Exception as e:
        logging.error(f"Failed to append data for date {date}: {e}")

# Decorators for rate limit + backoff
@sleep_and_retry
@limits(calls=RATE_LIMIT_CALLS, period=RATE_LIMIT_PERIOD)
def make_api_request(session: requests.Session, params: dict) -> requests.Response:
    """
    Make a rate-limited API request with retry on 429 errors.
    """
    response = session.get(BASE_URL, params=params, timeout=10)
    if response.status_code == 429:
        retry_after = int(response.headers.get("Retry-After", 60))
        logging.warning(f"Rate limit exceeded. Retrying after {retry_after} seconds.")
        time.sleep(retry_after)
        raise Exception("Rate limit exceeded")
    response.raise_for_status()
    return response

@backoff.on_exception(
    backoff.expo,
    (requests.exceptions.RequestException, Exception),
    max_tries=MAX_RETRIES,
    jitter=backoff.full_jitter,
    giveup=lambda e: isinstance(e, requests.exceptions.HTTPError)
                     and e.response is not None
                     and e.response.status_code < 500
)
def fetch_articles_for_date(session: requests.Session, date: str, max_pages: int = MAX_PAGES) -> List[str]:
    """
    Fetch articles for a specific date, handling pagination, rate limits, and backoff.
    """
    articles = []
    base_date = date.replace("-", "")
    for page in range(max_pages):
        params = {
            'q': QUERY,
            'begin_date': base_date,
            'end_date': base_date,
            'api-key': API_KEY,
            'page': page
        }
        try:
            logging.info(f"Request for date {date}, page {page}.")
            response = make_api_request(session, params)
            data = response.json()
            docs = data.get('response', {}).get('docs', [])
            if not docs:
                logging.info(f"No more articles found for date {date} on page {page}.")
                break
            for doc in docs:
                headline = doc.get('headline', {}).get('main')
                if headline:
                    articles.append(headline)
            if len(docs) < 10:
                logging.info(f"Less than 10 articles found for {date} page {page}, ending pagination.")
                break
        except Exception as e:
            logging.error(f"Request failed for date {date} page {page}: {e}")
            break
    return articles

def fetch_news_data(
    dates: List[str],
    output_file: str,
    max_pages_per_date: int = MAX_PAGES
) -> None:
    """
    Fetch news articles for a list of dates, saving them to the output CSV incrementally.
    """
    session = create_session()
    initialize_output_file(output_file)
    existing_dates = get_existing_dates(output_file)
    filtered_dates = [d for d in dates if d not in existing_dates]
    logging.info(f"Dates to process: {len(filtered_dates)} new dates.")

    if not filtered_dates:
        logging.info("No new dates to fetch.")
        return

    for date in tqdm(filtered_dates, desc="Fetching news data"):
        articles = fetch_articles_for_date(session, date, max_pages=max_pages_per_date)
        if articles:
            append_to_csv(output_file, date, articles)
        else:
            logging.warning(f"No articles found for date {date}.")

    logging.info(f"News fetching complete. Data appended in {output_file}.")


def run_news_subcommand(start_date_str: str) -> None:
    """
    Called by the 'news' subcommand to fetch news from NYT API for all stock CSV dates >= start_date.
    """
    # Load config to get stock_file, news_file, etc.
    config = load_config("config.json")
    stock_file = config['stock_file']
    news_file = config['news_file']

    start_dt = validate_start_date(start_date_str)

    # Get dates from the stock CSV that are >= start_dt
    dates = load_dates_from_stock_data(stock_file, start_dt)
    fetch_news_data(dates, news_file, max_pages_per_date=MAX_PAGES)

# ------------------------------------------------------------------------
# MAIN
# ------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Main script for entire pipeline: fetch, news, data, train, evaluate."
    )
    subparsers = parser.add_subparsers(dest='command', help='Sub-commands')

    #-------------------------------------------#
    # 1) FETCH (stock data) sub-command
    #-------------------------------------------#
    fetch_parser = subparsers.add_parser('fetch', help='Fetch stock data (yfinance + TA-Lib)')
=======
def main():
    parser = argparse.ArgumentParser(
        description="Main script that can fetch data, run data-setup, train, and evaluate."
    )
    subparsers = parser.add_subparsers(dest='command', help='Sub-commands')

    #-------------------------#
    # 1) FETCH sub-command
    #-------------------------#
    fetch_parser = subparsers.add_parser('fetch', help='Fetch stock data using yfinance + TA-Lib')
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
    fetch_parser.add_argument("-s", "--start", type=str, default="1985-01-01",
                              help="Start date (YYYY-MM-DD). Default=1985-01-01")
    fetch_parser.add_argument("-i", "--interval", type=str, default="1d",
                              help="Data interval (e.g., 1d, 15m). Default=1d")
    fetch_parser.add_argument("-c", "--config", type=str, default="config.json",
                              help="Path to config file. Default=config.json")
    fetch_parser.add_argument("-e", "--end", type=str, default=None,
                              help="End date (YYYY-MM-DD). Default=today")
    fetch_parser.add_argument("-t", "--ticker", type=str, default="^GSPC",
                              help="Ticker symbol (e.g. ^GSPC). Default=^GSPC")
    fetch_parser.add_argument("-o", "--output", type=str, default=None,
                              help="Output CSV path. Default=data/stocks/<ticker>.csv")

<<<<<<< HEAD
    #-------------------------------------------#
    # 2) NEWS sub-command
    #-------------------------------------------#
    news_parser = subparsers.add_parser('news', help='Fetch news articles from the NYT API')
    news_parser.add_argument("-s", "--start", type=str, default="2025-01-01",
                             help="Start date (YYYY-MM-DD) for news fetch. Default=2025-01-01")

    #-------------------------------------------#
    # 3) DATA sub-command
    #-------------------------------------------#
    data_setup_parser = subparsers.add_parser('data', help='Merge sentiment & create sequences')
    data_setup_parser.add_argument("--config", type=str, default="config.json",
                                   help="Path to the data config file.")

    #-------------------------------------------#
    # 4) TRAIN sub-command
    #-------------------------------------------#
    train_parser = subparsers.add_parser('train', help='Train the model')
    train_parser.add_argument("--config", type=str, default="config.json",
                              help="Path to the training config file.")

    #-------------------------------------------#
    # 5) EVALUATE sub-command
    #-------------------------------------------#
    eval_parser = subparsers.add_parser('evaluate', help='Evaluate the model')
    eval_parser.add_argument("--config", type=str, default="config.json",
=======
    #-------------------------#
    # 2) DATA-SETUP sub-command
    #-------------------------#
    data_setup_parser = subparsers.add_parser('data-setup', help='Merge sentiment & create sequences')
    data_setup_parser.add_argument("-c", "--config", type=str, default="config.json",
                                   help="Path to the data config file.")

    #-------------------------#
    # 3) TRAIN sub-command
    #-------------------------#
    train_parser = subparsers.add_parser('train', help='Train the model')
    train_parser.add_argument("-c", "--config", type=str, default="config.json",
                              help="Path to the training config file.")

    #-------------------------#
    # 4) EVALUATE sub-command
    #-------------------------#
    eval_parser = subparsers.add_parser('evaluate', help='Evaluate the model')
    eval_parser.add_argument("-c", "--config", type=str, default="config.json",
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
                             help="Path to the evaluation config file.")

    args = parser.parse_args()

<<<<<<< HEAD
    if not args.command:
=======
    if args.command is None:
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
        parser.print_help()
        sys.exit(1)

    if args.command == 'fetch':
<<<<<<< HEAD
        # We reuse stock.py's logic 
        cfg = load_config(args.config)
=======
        # Reuse the logic from stock.py's main but pass in arguments manually
        # Instead of rewriting logic, we directly call stock.fetch_stock_data 
        # or we can call stock.main() with sys.argv. We'll do direct call:
        config = load_config(args.config)

>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
        start_date = args.start
        interval = args.interval
        end_date = args.end
        ticker = args.ticker
<<<<<<< HEAD
        default_dir = cfg.get('data_dir', 'data/stocks')
        os.makedirs(default_dir, exist_ok=True)
        output_path = args.output if args.output else os.path.join(default_dir, f'{ticker}.csv')
=======
        # If user didn't provide output, auto-construct from config or default
        default_dir = config.get('data_dir', 'data')
        os.makedirs(default_dir, exist_ok=True)
        output_path = args.output if args.output else config['stock_file']
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3

        success = stock.fetch_stock_data(ticker, start_date, end_date, interval, output_path)
        if success:
            logging.info("Data fetch & processing completed successfully.")
        else:
<<<<<<< HEAD
            logging.error("Failed to fetch or process data.")
        sys.exit(0)

    elif args.command == 'news':
        # Our newly integrated news-collect logic
        run_news_subcommand(args.start)
        sys.exit(0)

    elif args.command == 'data':
=======
            logging.error("Failed to fetch/prepare data.")
        sys.exit(0)

    elif args.command == 'data-setup':
        # Let the data.py's main handle it
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
        data.main(args.config)
        sys.exit(0)

    elif args.command == 'train':
<<<<<<< HEAD
        train.main()
        sys.exit(0)

    elif args.command == 'evaluate':
        evaluate.main()
=======
        # Let train.py handle it
        train.train_model(args.config)
        sys.exit(0)

    elif args.command == 'evaluate':
        # Let evaluate.py handle it
        evaluate.main(["--config", args.config])
>>>>>>> 92e84d8b869c610628d9ca370ca20b57f60f4df3
        sys.exit(0)


if __name__ == "__main__":
    main()

