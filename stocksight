#!/usr/bin/env python
#================================
#  stocksight
#================================
"""
Orchestrates the entire pipeline:
    1) Fetch stock data (yfinance + TA-Lib)  [subcommand: stock]
    2) Fetch news data (NYT API)             [subcommand: news]
    3) Merge sentiment & create sequences    [subcommand: data]
    4) Train the model                       [subcommand: train]
    5) Evaluate the model                    [subcommand: evaluate]
    6) Predict future stock prices           [subcommand: predict]

Usage Examples:
    python main.py stock --ticker ^GSPC --start 2025-01-01 --end 2025-06-01
    python main.py news --start 2025-01-01
    python main.py data --config config.json --output data/merged_file.csv --cleaned /data/cleaned_file.csv --parallel
    python main.py train --config config.json
    python main.py evaluate --config config.json
    python main.py predict --config config.json --num-days 5
"""

import argparse
import logging
import sys
import os
import time
import json
import csv
from datetime import datetime
from typing import Dict, Any, List

import requests
import pandas as pd
from tqdm import tqdm

# ------------------------------------------------------------------------
# Global Logging Setup
# ------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

# ------------------------------------------------------------------------
# CONFIGURATION LOADING
# ------------------------------------------------------------------------
def load_config(config_path: str) -> Dict[str, Any]:
    """
    Load and return the configuration from a JSON file.
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    with open(config_path, 'r') as f:
        config = json.load(f)
    return config


# ------------------------------------------------------------------------
# MAIN
# ------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(
        description="Main script for entire pipeline: stock, news, data, train, evaluate, predict."
    )
    subparsers = parser.add_subparsers(dest='command', help='Sub-commands')

    #-------------------------------------------#
    # 1) STOCK (stock data) sub-command
    #-------------------------------------------#
    stock_parser = subparsers.add_parser('stock', help='Fetch stock data (yfinance + TA-Lib)')
    stock_parser.add_argument("-s", "--start", type=str, default="1985-01-01",
                              help="Start date (YYYY-MM-DD). Default=1985-01-01")
    stock_parser.add_argument("-i", "--interval", type=str, default="1d",
                              help="Data interval (e.g., 1d, 15m). Default=1d")
    stock_parser.add_argument("-c", "--config", type=str, default="config.json",
                              help="Path to config file. Default=config.json")
    stock_parser.add_argument("-e", "--end", type=str, default=None,
                              help="End date (YYYY-MM-DD). Default=today")
    stock_parser.add_argument("-t", "--ticker", type=str, default="^GSPC",
                              help="Ticker symbol (e.g. ^GSPC). Default=^GSPC")
    stock_parser.add_argument("-o", "--output", type=str, default=None,
                              help="Output CSV path. Default=data/stocks/<ticker>.csv")

    #-------------------------------------------#
    # 2) NEWS sub-command
    #-------------------------------------------#
    news_parser = subparsers.add_parser('news', help='Fetch news articles from the NYT API')
    news_parser.add_argument("-s", "--start", type=str, default="2024-12-20",
                             help="Start date (YYYY-MM-DD) for news fetch. Default=2024-12-20")

    #-------------------------------------------#
    # 3) DATA sub-command
    #-------------------------------------------#
    data_parser = subparsers.add_parser('data', help='Compute sentiments, merge data, create sequences')
    data_parser.add_argument("-c", "--config", type=str, default="config.json",
                              help="Path to the data config file.")
    data_parser.add_argument("-o", "--output", type=str, default=None,
                              help="Path to save the merged sentiments and price data.")
    data_parser.add_argument("-x", "--cleaned", type=str, default=None,
                              help="Path to save the cleaned CSV file for sentiments. If None, skip cleaning step.")
    data_parser.add_argument("-p", "--parallel", action='store_true',
                              help="Enable parallel processing for FinBERT sentiment analysis.")

    #-------------------------------------------------#
    # 4) TRAIN sub-command
    #-------------------------------------------------#
    train_parser = subparsers.add_parser('train', help='Train the model')
    train_parser.add_argument("-c", "--config", type=str, default="config.json",
                              help="Path to the training config file.")

    #-------------------------------------------------#
    # 5) EVALUATE sub-command
    #-------------------------------------------------#
    eval_parser = subparsers.add_parser('evaluate', help='Evaluate the model')
    eval_parser.add_argument("-c", "--config", type=str, default="config.json",
                              help="Path to the evaluation config file.")

    #-------------------------------------------------#
    # 6) PREDICT sub-command
    #-------------------------------------------------#
    predict_parser = subparsers.add_parser('predict', help='Predict future stock prices using the trained model')
    predict_parser.add_argument("-c", "--config", type=str, default="config.json",
                                 help="Path to the prediction config file.")
    predict_parser.add_argument("-n", "--num-days", type=int, default=5,
                                 help="Number of future days to predict. Default=5")

    #-------------------------------------------------#
    # Parse and dispatch
    #-------------------------------------------------#
    args = parser.parse_args()
    if not args.command:
        parser.print_help()
        sys.exit(1)

    #---------------- STOCK sub-command ----------------#
    if args.command == 'stock':
        import utils.stock as stock  # stock.py
        cfg = load_config(args.config)
        start_date = args.start
        interval = args.interval
        end_date = args.end
        ticker = args.ticker
        default_dir = cfg.get('data_dir', 'data')
        os.makedirs(default_dir, exist_ok=True)
        output_path = args.output if args.output else cfg['price_file']
        success = stock.fetch_price_data(ticker, start_date, end_date, interval, output_path)
        if success:
            logging.info("Stock data fetch & processing completed successfully.")
        else:
            logging.error("Failed to fetch or process stock data.")
        sys.exit(0)

    #---------------- NEWS sub-command ----------------#
    elif args.command == 'news':
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import backoff
        from ratelimit import limits, sleep_and_retry
        API_KEY = os.getenv("NYT_API_KEY")
        if not API_KEY:
            logging.error("NYT_API_KEY environment variable not set.")
            raise EnvironmentError("Please set the NYT_API_KEY environment variable to access the NYT API.")
        BASE_URL = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
        QUERY = "finance business technology"
        MAX_RETRIES = 2
        BACKOFF_FACTOR = 2
        MAX_PAGES = 10
        RATE_LIMIT_CALLS = 8
        RATE_LIMIT_PERIOD = 60
        def create_session() -> requests.Session:
            session = requests.Session()
            retries = Retry(
                total=MAX_RETRIES,
                backoff_factor=BACKOFF_FACTOR,
                status_forcelist=[500, 502, 503, 504],
                allowed_methods=["GET"]
            )
            adapter = HTTPAdapter(max_retries=retries)
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session
        def is_valid_date(date_str: str) -> bool:
            try:
                datetime.strptime(date_str, "%Y-%m-%d")
                return True
            except ValueError:
                return False
        def validate_start_date(start_date_str: str) -> datetime:
            try:
                start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
                return start_date
            except ValueError:
                logging.error("start_date must be in 'YYYY-MM-DD' format.")
                raise
        def load_dates_from_stock_data(price_file: str, start_date_dt: datetime) -> List[str]:
            try:
                stock_data = pd.read_csv(price_file)
                if 'Date' not in stock_data.columns:
                    logging.error("Stock data CSV does not contain 'Date' column.")
                    raise KeyError("Missing 'Date' column.")
                valid_dates = []
                for d in stock_data['Date'].dropna().unique():
                    if is_valid_date(str(d)):
                        dt_obj = datetime.strptime(str(d), "%Y-%m-%d")
                        if dt_obj >= start_date_dt:
                            valid_dates.append(dt_obj)
                valid_dates.sort()
                return [dt.strftime("%Y-%m-%d") for dt in valid_dates]
            except Exception as e:
                logging.error(f"Failed to load dates from {price_file}: {e}")
                raise
        def initialize_output_file(output_file: str) -> None:
            if not os.path.exists(output_file):
                try:
                    with open(output_file, 'w', encoding='utf-8', newline='') as f:
                        writer = csv.writer(f)
                        writer.writerow(['Date', 'Articles'])
                    logging.info(f"Created new output file with headers: {output_file}")
                except Exception as e:
                    logging.error(f"Failed to initialize output file {output_file}: {e}")
                    raise
        def get_existing_dates(output_file: str) -> set:
            existing_dates = set()
            if os.path.exists(output_file):
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            date_str = row.get('Date', '').strip()
                            if is_valid_date(date_str):
                                existing_dates.add(date_str)
                    logging.info(f"Loaded {len(existing_dates)} existing dates from {output_file}.")
                except Exception as e:
                    logging.error(f"Failed to read existing output file {output_file}: {e}")
            return existing_dates
        def append_to_csv(output_file: str, date: str, articles: List[str]) -> None:
            articles_str = json.dumps(articles, ensure_ascii=False)
            try:
                with open(output_file, 'a', encoding='utf-8', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([date, articles_str])
                logging.info(f"Appended {len(articles)} articles for date {date} -> {output_file}.")
            except Exception as e:
                logging.error(f"Failed to append data for date {date}: {e}")
        @sleep_and_retry
        @limits(calls=RATE_LIMIT_CALLS, period=RATE_LIMIT_PERIOD)
        def make_api_request(session: requests.Session, params: dict) -> requests.Response:
            response = session.get(BASE_URL, params=params, timeout=10)
            if response.status_code == 429:
                retry_after = int(response.headers.get("Retry-After", 60))
                logging.warning(f"Rate limit exceeded. Retrying after {retry_after} seconds.")
                time.sleep(retry_after)
                raise Exception("Rate limit exceeded")
            response.raise_for_status()
            return response
        @backoff.on_exception(
            backoff.expo,
            (requests.exceptions.RequestException, Exception),
            max_tries=MAX_RETRIES,
            jitter=backoff.full_jitter,
            giveup=lambda e: isinstance(e, requests.exceptions.HTTPError)
                            and e.response is not None
                            and e.response.status_code < 500
        )
        def fetch_articles_for_date(session: requests.Session, date: str, max_pages: int = MAX_PAGES) -> List[str]:
            articles = []
            base_date = date.replace("-", "")
            for page in range(max_pages):
                params = {
                    'q': QUERY,
                    'begin_date': base_date,
                    'end_date': base_date,
                    'api-key': API_KEY,
                    'page': page
                }
                try:
                    logging.info(f"Request for date {date}, page {page}.")
                    response = make_api_request(session, params)
                    data = response.json()
                    docs = data.get('response', {}).get('docs', [])
                    if not docs:
                        logging.info(f"No more articles found for date {date} on page {page}.")
                        break
                    for doc in docs:
                        headline = doc.get('headline', {}).get('main')
                        if headline:
                            articles.append(headline)
                    if len(docs) < 10:
                        logging.info(f"Less than 10 articles found for {date} page {page}, ending pagination.")
                        break
                except Exception as e:
                    logging.error(f"Request failed for date {date} page {page}: {e}")
                    break
            return articles
        def fetch_news_data(
            dates: List[str],
            output_file: str,
            max_pages_per_date: int = MAX_PAGES
        ) -> None:
            session = create_session()
            initialize_output_file(output_file)
            existing_dates = get_existing_dates(output_file)
            filtered_dates = [d for d in dates if d not in existing_dates]
            logging.info(f"Dates to process: {len(filtered_dates)} new dates.")
            if not filtered_dates:
                logging.info("No new dates to fetch.")
                return
            for date in tqdm(filtered_dates, desc="Fetching news data"):
                articles = fetch_articles_for_date(session, date, max_pages=max_pages_per_date)
                if articles:
                    append_to_csv(output_file, date, articles)
                else:
                    logging.warning(f"No articles found for date {date}.")
            logging.info(f"News fetching complete. Data appended in {output_file}.")
        def run_news_subcommand(start_date_str: str) -> None:
            config = load_config("config.json")
            price_file = config['price_file']
            news_file = config['news_file']
            start_dt = validate_start_date(start_date_str)
            dates = load_dates_from_stock_data(price_file, start_dt)
            fetch_news_data(dates, news_file, max_pages_per_date=MAX_PAGES)
        run_news_subcommand(args.start)
        sys.exit(0)

    #---------------- DATA sub-command ----------------#
    elif args.command == 'data':
        # # GPU Configuration: Use tensorflow-directml if available (for Intel Xe Graphics)
        # try:
        #     import tensorflow_directml_plugin as tf
        #     logging.info("Using tensorflow-directml for GPU acceleration.")
        # except ImportError:
        #     import tensorflow as tf
        #     logging.info("Using standard TensorFlow.")
        #
        #
        # def configure_intel_gpu():
        #     """
        #     Configure TensorFlow to use the Intel Xe Graphics GPU if available.
        #     If tensorflow-directml is used, GPU support is enabled automatically.
        #     Otherwise, try to enable memory growth on detected GPUs.
        #     """
        #     try:
        #         gpus = tf.config.list_physical_devices('GPU')
        #         if gpus:
        #             for gpu in gpus:
        #                 details = tf.config.experimental.get_device_details(gpu)
        #                 device_name = details.get('device_name', gpu.name)
        #                 if "Intel" in device_name or "Xe" in device_name:
        #                     logging.info(f"Using Intel GPU: {device_name}")
        #                     try:
        #                         tf.config.experimental.set_memory_growth(gpu, True)
        #                     except RuntimeError as e:
        #                         logging.error(f"Error setting memory growth on GPU: {e}")
        #                 else:
        #                     logging.info(f"Found GPU but not Intel Xe: {device_name}")
        #         else:
        #             logging.info("No GPU found. Using CPU.")
        #     except Exception as e:
        #         logging.error(f"Error during GPU configuration: {e}")
        #
        #
        # # Configure Intel Xe Graphics GPU if available
        # configure_intel_gpu()

        import utils.data as data  # data.py in the current directory
        logging.info("Starting sentiment computation step (FinBERT) before data setup...")
        cfg = load_config(args.config)
        try:
            import utils.sentiments as sentiments  # sentiments.py should handle FinBERT sentiment analysis
            sentiments.compute_sentiment(
                input_file=cfg['news_file'],
                output_file=cfg['sentiment_file'],
                cleaned_file=args.cleaned,
                batch_size=cfg.get('batch_size', 32),
                parallel=args.parallel
            )
            logging.info("Sentiment computation completed successfully.")
        except Exception as e:
            logging.error(f"Sentiment computation failed: {e}")
            sys.exit(1)
        logging.info("Proceeding with standard data setup (merging, sequences)...")
        data.main(config_path=args.config)
        sys.exit(0)

    #---------------- TRAIN sub-command ----------------#
    elif args.command == 'train':
        # GPU Configuration: Use tensorflow-directml if available (for Intel Xe Graphics)
        try:
            import tensorflow_intel as tf
            logging.info("Using tensorflow-intel for GPU acceleration.")
        except ImportError:
            import tensorflow as tf
            logging.info("Using standard TensorFlow.")


        def configure_intel_gpu():
            """
            Configure TensorFlow to use the Intel Xe Graphics GPU if available.
            If tensorflow-directml is used, GPU support is enabled automatically.
            Otherwise, try to enable memory growth on detected GPUs.
            """
            try:
                gpus = tf.config.list_physical_devices('GPU')
                if gpus:
                    for gpu in gpus:
                        details = tf.config.experimental.get_device_details(gpu)
                        device_name = details.get('device_name', gpu.name)
                        if "Intel" in device_name or "Xe" in device_name:
                            logging.info(f"Using Intel GPU: {device_name}")
                            try:
                                tf.config.experimental.set_memory_growth(gpu, True)
                            except RuntimeError as e:
                                logging.error(f"Error setting memory growth on GPU: {e}")
                        else:
                            logging.info(f"Found GPU but not Intel Xe: {device_name}")
                else:
                    logging.info("No GPU found. Using CPU.")
            except Exception as e:
                logging.error(f"Error during GPU configuration: {e}")


        # Configure Intel Xe Graphics GPU if available
        configure_intel_gpu()

        import utils.train_model as train
        train.main(args.config)
        sys.exit(0)

    #---------------- EVALUATE sub-command ----------------#
    elif args.command == 'evaluate':
        import utils.evaluate_model as evaluate
        evaluate.main()
        sys.exit(0)

    #---------------- PREDICT sub-command ----------------#
    elif args.command == 'predict':
        import utils.predict_future_price as predict
        predict.main(config_path=args.config, num_days=args.num_days)
        sys.exit(0)


if __name__ == "__main__":
    main()

