#================================
#  data.py
#================================

import os
import logging
import numpy as np
import pandas as pd
import argparse
import joblib

import numpy as np
import pandas as pd
from .utils import (
    load_config,
    validate_files,
    scale_data,
    save_scalers,
    log_time
)


@log_time
def merge_sentiment_with_stock(price_file: str, sentiment_file: str, output_file: str) -> None:
    """
    Merge stock data with sentiment data and save the merged dataset.

    Parameters:
        price_file (str): Path to the price data CSV file.
        sentiment_file (str): Path to the sentiment data CSV file.
        output_file (str): Path where the merged data will be saved.
    """
    logging.info(f"Merging '{price_file}' with sentiment '{sentiment_file}' -> '{output_file}'")

    # Validate input files
    validate_files([price_file, sentiment_file])

    # Load datasets
    try:
        stock_data = pd.read_csv(price_file, parse_dates=['Date'])
        logging.info(f"Loaded stock data from {price_file}")
    except Exception as e:
        logging.error(f"Failed to load stock data: {e}")
        raise

    try:
        sentiment_data = pd.read_csv(sentiment_file, parse_dates=['Date'])
        logging.info(f"Loaded sentiment data from {sentiment_file}")
    except Exception as e:
        logging.error(f"Failed to load sentiment data: {e}")
        raise

    # Merge on Date (Broadcast daily sentiment to intraday stock rows if needed)
    try:
        # Create temporary normalize date columns for merging
        stock_data['merge_date'] = stock_data['Date'].dt.normalize()
        sentiment_data['merge_date'] = sentiment_data['Date'].dt.normalize()
        
        # Merge on the normalized date
        merged_data = pd.merge(stock_data, sentiment_data, on='merge_date', how='left', suffixes=('', '_sent'))
        
        # Drop the temporary merge column and any duplicate date column from sentiment if it exists
        merged_data.drop(columns=['merge_date'], inplace=True)
        if 'Date_sent' in merged_data.columns:
            merged_data.drop(columns=['Date_sent'], inplace=True)
            
        logging.info("Merged data on normalized date (broadcasting sentiment)")
    except KeyError as e:
        logging.error(f"Merge failed: {e}")
        raise

    # Fill missing sentiment data with neutral values
    fill_values = {
        'Positive': 0,
        'Negative': 0,
        'Neutral': 1,
        'Aggregate_Score': 0
    }
    merged_data.fillna(fill_values, inplace=True)
    logging.info("Filled missing sentiment data with default neutral values.")

    # Apply Feature Engineering
    merged_data = feature_engineer(merged_data)

    # Ensure output directory exists
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        logging.info(f"Created output directory: {output_dir}")

    # Save merged dataset
    merged_data.to_csv(output_file, index=False)
    logging.info(f"Merged dataset saved to {output_file}")


def feature_engineer(df: pd.DataFrame) -> pd.DataFrame:
    """
    Add engineered features to the dataframe.
    
    Adds:
    - Log_Return: Logarithmic return of Close price.
    - Rolling_Std_20: 20-day rolling standard deviation of Close price.
    - Close_Lag_1: Previous day's Close price.
    """
    logging.info("Starting feature engineering...")
    
    # Avoid SettingWithCopyWarning
    df = df.copy()
    
    # 1. Log Return
    # Using small epsilon to avoid log(0) if Close is 0 (unlikely for stocks but good practice)
    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    
    # 2. Rolling Volatility (Std Dev)
    df['Rolling_Std_20'] = df['Close'].rolling(window=20).std()
    
    # 3. Lagged Features
    df['Close_Lag_1'] = df['Close'].shift(1)
    
    # Drop rows with NaNs generated by shifting/rolling
    original_len = len(df)
    df.dropna(inplace=True)
    dropped_rows = original_len - len(df)
    
    logging.info(f"Feature engineering completed. Dropped {dropped_rows} rows due to NaN values.")
    
    return df



@log_time
def create_sequences_with_sentiment(
    input_file: str,
    output_file: str,
    feature_columns: list,
    window_size: int,
    target_column: str = 'Close'
) -> None:
    """
    Create sequences for time-series data (including sentiment columns)
    and save to a .npz file. Data is NOT scaled here.

    Parameters:
        input_file (str): Path to the merged CSV file.
        output_file (str): Path where the sequences will be saved (.npz).
        feature_columns (list): List of feature column names.
        window_size (int): The number of timesteps per sequence.
        target_column (str): The column to predict.
    """
    if not os.path.exists(input_file):
        logging.error(f"Input file {input_file} does not exist.")
        raise FileNotFoundError(f"Input file {input_file} not found.")

    data = pd.read_csv(input_file)
    logging.info(f"Loaded data for sequence creation from {input_file}")

    # Define the column we want to predict
    target_column_name = target_column
    if target_column_name not in data.columns:
        raise ValueError(f"Target column '{target_column_name}' not found in data.")

    # Check for missing feature columns
    missing_cols = set(feature_columns) - set(data.columns)
    if missing_cols:
        logging.error(f"Missing columns in data: {missing_cols}")
        raise ValueError(f"Missing columns: {missing_cols}")

    # Extract raw features and target
    features_raw = data[feature_columns].values
    
    # The index of 'Close' among the chosen feature columns
    target_idx = feature_columns.index(target_column_name)

    # Build sequences
    X = np.array([
        features_raw[i : i + window_size]
        for i in range(len(features_raw) - window_size)
    ])
    y = features_raw[window_size:, target_idx]

    # Ensure output directory exists
    output_dir = os.path.dirname(output_file)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # Save sequences and labels
    np.savez(output_file, features=X, labels=y)
    logging.info(f"Raw sequences saved to {output_file}")


def main(config_path: str) -> None:
    """
    Orchestrates the data preparation steps:
    1) Merge stock data with sentiment data
    2) Create scaled sequences for model training
    """
    try:
        config = load_config(config_path)
    except Exception as e:
        logging.error(f"Failed to load configuration: {e}")
        raise

    # Validate necessary config keys
    required_keys = [
        'price_file', 'sentiment_file', 'merged_file',
        'sequence_file', 'feature_columns', 'window_size',
        'scaler_file_X', 'scaler_file_y'
    ]
    missing_keys = [k for k in required_keys if k not in config]
    if missing_keys:
        logging.error(f"Missing configuration keys: {missing_keys}")
        raise KeyError(f"Missing configuration keys: {missing_keys}")

    # Step 1: Merge data
    merge_sentiment_with_stock(
        price_file=config['price_file'],
        sentiment_file=config['sentiment_file'],
        output_file=config['merged_file']
    )

    # Step 2: Create sequences
    feature_cols_extended = config['feature_columns']

    create_sequences_with_sentiment(
        input_file=config['merged_file'],
        output_file=config['sequence_file'],
        feature_columns=feature_cols_extended,
        window_size=config['window_size'],
        target_column=config.get('target_column', 'Close')
    )


if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler("logs/data_setup.log"),
            logging.StreamHandler()
        ]
    )

    parser = argparse.ArgumentParser(description="End-to-end data setup script.")
    parser.add_argument(
        "-c",
        "--config",
        type=str,
        default="config.json",
        help="Path to the configuration JSON file."
    )
    args = parser.parse_args()

    main(args.config)

